<div align="center">

# ğŸš€ MechGaia AgentBeats

**A standardized agent assessment framework for evaluating AI agents on mechanical engineering problems**

[![Python](https://img.shields.io/badge/Python-3.13+-blue.svg)](https://www.python.org/downloads/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![A2A](https://img.shields.io/badge/A2A-0.3.8+-orange.svg)](https://github.com/a2a-sdk)
[![MCP](https://img.shields.io/badge/MCP-Compatible-purple.svg)](https://modelcontextprotocol.io/)

Built using **A2A (Agent-to-Agent)** and **MCP (Model Context Protocol)** standards, this framework tests agents' ability to solve engineering problems using scientific computing tools (calculator, Python scipy, numpy, etc.).

---

### ğŸ¯ Quick Actions

<a href="#quick-start"><img src="https://img.shields.io/badge/ğŸš€ Quick Start-Get Started-blue?style=for-the-badge" alt="Quick Start"></a>
<a href="#installation"><img src="https://img.shields.io/badge/ğŸ“¦ Installation-Setup-green?style=for-the-badge" alt="Installation"></a>
<a href="#task-levels"><img src="https://img.shields.io/badge/ğŸ“Š Task Levels-View Details-orange?style=for-the-badge" alt="Task Levels"></a>
<a href="#usage"><img src="https://img.shields.io/badge/ğŸ’» Usage-Examples-purple?style=for-the-badge" alt="Usage"></a>

---

</div>

## ğŸ“‘ Table of Contents

- [Overview](#overview)
- [Architecture](#architecture)
- [Installation](#installation)
- [Quick Start](#quick-start)
- [Task Levels](#task-levels)
- [Evaluation Process](#evaluation-process)
- [Usage](#usage)
- [Configuration](#configuration)
- [Testing](#testing)
- [Troubleshooting](#troubleshooting)

## ğŸ“– Overview

MechGaia AgentBeats evaluates AI agents on their ability to solve mechanical engineering problems across **four difficulty levels (A through D)**. The framework uses a distributed agent architecture where:

<div align="center">

| Component | Role | Description |
|-----------|------|-------------|
| ğŸŸ¢ **Green Agent** | Assessment Manager | Hosts the MechGAIA benchmark, presents tasks, and evaluates responses |
| âšª **White Agent** | Target Agent | The agent being tested (typically an LLM-powered agent) |
| ğŸš€ **Launcher** | Orchestrator | Orchestrates the evaluation workflow, starting agents and coordinating the assessment |

</div>

### âœ¨ Key Features

- âœ… **Quantitative Evaluation**: Unit tests, constraint satisfaction, numerical accuracy
- âœ… **Qualitative Evaluation**: LLM-as-Judge scoring for comprehensive assessment
- âœ… **Multi-Level Testing**: Four progressive difficulty levels (A â†’ D)
- âœ… **Scientific Computing**: Integration with Python scipy, numpy, and engineering tools
- âœ… **Automated Reporting**: Statistical analysis and result visualization

## ğŸ—ï¸ Architecture

### System Components

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Launcher  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ Green Agent  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚White Agent  â”‚
â”‚  (orchestrates) â”‚     â”‚ (Evaluator)  â”‚         â”‚ (LLM being  â”‚
â”‚             â”‚         â”‚              â”‚         â”‚  tested)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚   Database   â”‚
                        â”‚  (Results)   â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

<div align="center">

**ğŸ”„ Communication Flow**

</div>

### Key Definitions

- **Green Agent**: The evaluator/assessor agent that manages the benchmark, presents tasks to the white agent, and evaluates responses using both automated tests and LLM-as-Judge scoring.

- **White Agent**: The agent being tested. It receives engineering problems and must solve them using available tools (calculator, Python execution, etc.). The white agent communicates via A2A protocol.

- **Task**: A mechanical engineering problem with a specific schema and objectives. Tasks are stored in a SQLite database.

- **Task Instance**: A specific instantiation of a task with particular parameters. Each task can have multiple instances with different parameter values.

- **Level**: Task difficulty classification (A, B, C, or D) representing increasing complexity:
  - **Level A**: Multiple-choice conceptual questions
  - **Level B**: Parametric calculation problems
  - **Level C**: Single-component design optimization
  - **Level D**: Multi-component system design

- **Evaluation**: The process of presenting a task to the white agent, collecting its response, and scoring it using quantitative (unit tests) and qualitative (LLM-as-Judge) methods.

## ğŸ“¦ Installation

### Prerequisites

<div align="center">

| Requirement | Version | Notes |
|-------------|---------|-------|
| ğŸ **Python** | 3.13+ | Required |
| ğŸ“¦ **uv** | Latest | Recommended package manager |
| ğŸ”„ **pip** | Latest | Alternative to uv |

</div>

### Setup

<details>
<summary><b>ğŸ“‹ Step-by-Step Installation Guide</b></summary>

#### 1ï¸âƒ£ Clone the repository:
```bash
git clone <repository-url>
cd mechgaia-agentbeats
```

#### 2ï¸âƒ£ Install dependencies:
```bash
uv sync
```

#### 3ï¸âƒ£ Set up environment variables (see [Configuration](#configuration))

</details>

<div align="center">

**âš¡ Quick Install**

```bash
git clone <repository-url> && cd mechgaia-agentbeats && uv sync
```

</div>

## ğŸš€ Quick Start

<div align="center">

**Get up and running in 4 simple steps!**

</div>

### Step 1: Generate Tasks (One-time Setup)

<div align="center">

<details>
<summary><b>ğŸ“ Click to expand task generation commands</b></summary>

</div>

Before running evaluations, you need to populate the database with tasks:

```bash
# Generate tasks for all levels
python scripts/generate_tasks.py generate --level A --num-tasks 5 --num-instances 10
python scripts/generate_tasks.py generate --level B --num-tasks 5 --num-instances 10
python scripts/generate_tasks.py generate --level C --num-tasks 3 --num-instances 10

# Level D tasks are loaded from example files
python scripts/generate_tasks.py generate --level D --examples-dir data/level_d/examples
```

</details>

### Step 2: Set Environment Variables

<div align="center">

<details>
<summary><b>ğŸ” Click to expand environment configuration</b></summary>

</div>

Create a `.env` file in the project root:

```bash
# OpenAI API Configuration
# Both green and white agents use the same API key
OPENAI_API_KEY=sk-your-openai-api-key-here

# Optional: for litellm proxy
LITELLM_PROXY_API_KEY=your-proxy-key

# Optional: Agent URL Configuration (defaults to localhost if not set)
AGENT_URL_GREEN=http://localhost:9001
AGENT_URL_WHITE=http://localhost:9002
```

</details>

### Step 3: Run the Benchmark

<div align="center">

**Choose your preferred method:**

</div>

<details>
<summary><b>âœ… Option A: All-in-one (Recommended for First Time)</b></summary>

This automatically starts both agents and runs the evaluation:

```bash
# Default: Runs one instance of Level C for testing
uv run python main.py launch

# Run a specific level
uv run python main.py launch --level A

# Run multiple levels
uv run python main.py launch --levels A,B,C
```

</details>

<details>
<summary><b>âš™ï¸ Option B: Manual (For Production/Testing)</b></summary>

**Terminal 1** - Start Green Agent (Evaluator):
```bash
uv run python main.py green
```

**Terminal 2** - Start White Agent (LLM being tested):
```bash
uv run python main.py white
```

**Terminal 3** - Run Evaluation:
```bash
# Evaluate Level A tasks
uv run python main.py launch_remote http://localhost:9001 http://localhost:9002 --level A

# Evaluate Level B tasks  
uv run python main.py launch_remote http://localhost:9001 http://localhost:9002 --level B

# Evaluate specific model
uv run python main.py launch_remote http://localhost:9001 http://localhost:9002 --level A --model "anthropic/claude-3-opus"
```

</details>

### Step 4: View Results

<div align="center">

```bash
# Generate reports
python scripts/analyze_results.py analyze --output-dir results

# View markdown report
cat results/report.md
```

</div>

## ğŸ“Š Task Levels

<div align="center">

**The framework evaluates agents across four progressively challenging levels:**

| Level | Type | Complexity | Focus |
|-------|------|------------|-------|
| **A** | Multiple-Choice | â­ Basic | Conceptual Understanding |
| **B** | Parametric Calculation | â­â­ Intermediate | Numerical Problem Solving |
| **C** | Single-Component Design | â­â­â­ Advanced | Design Optimization |
| **D** | Multi-Component System | â­â­â­â­ Expert | System-Level Design |

</div>

### ğŸ“ Level A: Multiple-Choice Conceptual Questions

<div align="center">

**â­ Basic Level - Conceptual Understanding**

</div>

<details>
<summary><b>ğŸ“‹ View Details</b></summary>

**Description**: Tests fundamental understanding of mechanical engineering concepts through multiple-choice questions.

**Characteristics**:
- âœ… Conceptual understanding questions
- âœ… No calculations required
- âœ… Focus on engineering principles and theory
- âœ… Requires explanation and distractor analysis

**Example Topics**:
- ğŸ”¬ Material properties and behavior
- ğŸ“ Stress and strain concepts
- ğŸ—ï¸ Beam theory fundamentals
- âš¡ Vibration and dynamics principles

**Response Format**: Agent selects an option and provides reasoning.

**Evaluation Criteria**:
- âœ… **Correctness** (binary: correct/incorrect option selection)
- ğŸ“Š **Technical Accuracy** (1-5): Correct understanding of engineering concepts
- ğŸ“ **Conceptual Clarity** (1-5): Clear and well-structured explanation
- ğŸ” **Distractor Analysis** (1-5): Explanation of why selected option is correct and others are wrong
- ğŸ§  **Reasoning Quality** (1-5): Sound engineering reasoning and logical thought process

</details>

### ğŸ”¢ Level B: Parametric Calculation Problems

<div align="center">

**â­â­ Intermediate Level - Numerical Problem Solving**

</div>

<details>
<summary><b>ğŸ“‹ View Details</b></summary>

**Description**: Tests ability to perform engineering calculations with given parameters.

**Characteristics**:
- ğŸ”¢ Numerical problems with specific input parameters
- ğŸ› ï¸ Requires tool usage (calculator, Python execution)
- ğŸ“ Focus on correct application of formulas
- ğŸ“ Unit handling and conversion

**Example Topics**:
- ğŸ“Š Beam deflection calculations
- âš™ï¸ Stress analysis
- ğŸŒŠ Natural frequency computation
- ğŸ”¬ Material property calculations

**Response Format**: Agent provides numerical answer, optionally with code and explanation.

**Evaluation Criteria**:

**Quantitative (UnitTestGrader)**:
- âœ… **Correctness**: Numerical accuracy within Â±1% tolerance
- âœ… **Value Tolerance**: Pass/fail based on Â±1% error threshold
- âœ… **Unit Consistency**: Proper unit handling and conversion
- âœ… **Code Execution**: Successful execution of provided code
- âœ… **Intermediate Logic**: Correct calculation steps

**Qualitative (LLMJudgeGrader)**:
- ğŸ“Š **Technical Accuracy** (1-5): Correct governing equations and sound physics
- ğŸ§® **Mathematical Rigor** (1-5): Accurate calculations and proper unit handling
- ğŸ¯ **Problem-Solving Approach** (1-5): Logical and well-structured solution method
- ğŸ—ï¸ **Engineering Judgment** (1-5): Physically reasonable results and engineering context awareness

</details>

### ğŸ¯ Level C: Single-Component Design Optimization

<div align="center">

**â­â­â­ Advanced Level - Design Optimization**

</div>

<details>
<summary><b>ğŸ“‹ View Details</b></summary>

**Description**: Tests ability to design and optimize a single mechanical component with multiple constraints.

**Characteristics**:
- ğŸ¨ Design optimization problems
- ğŸ”’ Multiple constraints (deflection, stress, frequency, mass)
- ğŸ“ Requires selecting design variables (geometry, material)
- ğŸ’» Must provide executable code to verify design

**Example Topics**:
- ğŸ—ï¸ Beam design (height, width, material selection)
- ğŸŒŠ Frequency optimization
- âœ… Constraint satisfaction
- âš–ï¸ Trade-off analysis

**Response Format**: Agent provides:
- ğŸ“‹ Design parameters (JSON object with geometric and material choices)
- ğŸ“ Rationale (2-6 sentences explaining trade-offs)
- ğŸ’» Executable Python code to recompute design metrics

**Evaluation Criteria**:

**Quantitative**:
- âœ… **Constraint Satisfaction**: All constraints met (deflection, stress, frequency, mass)
- âœ… **Code Execution**: Code runs successfully and produces expected outputs
- âœ… **Syntax Correctness**: Valid Python syntax

**Qualitative (LLMJudgeGrader)**:
- ğŸ“Š **Technical Accuracy** (1-5): Correct identification of key relationships (e.g., fâ‚ âˆ âˆš(EI/(ÏALâ´)))
- ğŸ›¡ï¸ **Safety/Constraint Awareness** (1-5): Addresses safety factors, manufacturing constraints, cost
- ğŸ§  **Reasoning Quality** (1-5): Explains design choices and scaling relationships
- ğŸ—ï¸ **Engineering Judgment** (1-5): Practical, manufacturable design with considered trade-offs

</details>

### ğŸ—ï¸ Level D: Multi-Component System Design

<div align="center">

**â­â­â­â­ Expert Level - System-Level Design**

</div>

<details>
<summary><b>ğŸ“‹ View Details</b></summary>

**Description**: Tests ability to design multi-component mechanical systems with complex interactions.

**Characteristics**:
- ğŸ”— Multi-component systems (e.g., two-span beams, frames, shaft systems)
- ğŸ“Š System-level analysis (deflections, stresses, frequencies across components)
- ğŸ“ Multiple design variables per component
- ğŸ¨ Material selection and geometry optimization
- ğŸ”’ System-level constraints

**Example Topics**:
- ğŸŒ‰ Two-span continuous beam systems
- ğŸ¢ Multi-story portal frames
- âš™ï¸ Shaft systems with multiple segments
- ğŸ’° Cost-optimal designs

**Response Format**: Agent provides:
- ğŸ“‹ Design parameters for all components
- ğŸ“Š System metrics (deflections, stresses, frequencies, masses)
- ğŸ“ Rationale explaining multi-step decision-making
- ğŸ’» Executable Python code for system verification

**Evaluation Criteria**:

**Quantitative**:
- âœ… **System Constraint Satisfaction**: All system-level constraints met
- âœ… **Code Execution**: Code executes and produces system metrics
- âœ… **Component Coordination**: Consistent design across components

**Qualitative (LLMJudgeGrader)**:
- ğŸ“Š **Technical Accuracy** (1-5): Correct handling of multi-component interactions
- ğŸ”— **Multi-Step Coordination** (1-5): All steps addressed, consistent decisions
- ğŸ¯ **System Constraint Awareness** (1-5): System-level constraints properly addressed
- ğŸ—ï¸ **Engineering Judgment** (1-5): Practical system design with well-reasoned trade-offs

> **âš ï¸ Note**: Level D tasks use simplified engineering formulas. Small metric differences (Â±10-15%) between reference and verifier outputs are acceptable due to approximation differences.

</details>

## ğŸ”„ Evaluation Process

### ğŸ“‹ Workflow

<div align="center">

**7-step evaluation workflow**

</div>

1. **ğŸ“ Task Selection**: The launcher determines which tasks to evaluate based on:
   - âœ… Explicit level specification (`--level` or `--levels`)
   - âœ… Default behavior: one instance of Level C (for testing)
   - âœ… Database contents (auto-detects available levels)

2. **ğŸš€ Agent Initialization**:
   - ğŸŸ¢ Green agent starts on port 9001
   - âšª White agent starts on port 9002
   - âœ… Both agents register and become ready

3. **ğŸ“‹ Task Presentation**:
   - ğŸ—„ï¸ Green agent loads task from database
   - ğŸ“ Presents problem statement to white agent
   - ğŸ› ï¸ Provides available tools (calculator, Python execution, etc.)

4. **ğŸ’¬ Agent Response**:
   - ğŸ¤– White agent solves problem using tools
   - ğŸ“Š Responds with answer, code, and/or explanation
   - ğŸ“ Format depends on task level (see [Task Levels](#task-levels))

5. **âœ… Evaluation**:
   - **ğŸ”¢ Quantitative Evaluation**: UnitTestGrader checks:
     - âœ… Numerical correctness (within tolerance)
     - âœ… Code execution success
     - âœ… Constraint satisfaction
   
   - **ğŸ“Š Qualitative Evaluation**: LLMJudgeGrader (Mechanical Engineering Judge) scores:
     - ğŸ“ Technical accuracy
     - ğŸ§® Mathematical rigor
     - ğŸ—ï¸ Engineering judgment
     - ğŸ§  Reasoning quality

6. **ğŸ’¾ Result Storage**:
   - ğŸ—„ï¸ Scores stored in SQLite database (`data/benchmark.db`)
   - ğŸ“Š Results include: task ID, instance ID, model name, scores, timestamps

7. **ğŸ“ˆ Report Generation**:
   - ğŸ“Š Use `scripts/analyze_results.py` to generate statistical reports
   - ğŸ“ˆ Reports include mean scores, confidence intervals, per-level breakdowns

### ğŸ“Š Scoring Methodology

<div align="center">

**Two-pronged evaluation approach**

</div>

**ğŸ”¢ Quantitative Scoring**:
- Level B: Â±1% tolerance for numerical answers
- Level C/D: Binary constraint satisfaction (met/not met)
- Code execution: Pass/fail based on successful execution

**ğŸ“Š Qualitative Scoring**:
- LLM-as-Judge (Mechanical Engineering Judge) evaluates responses
- Scores range from 1-5 for each criterion
- Normalized to 0-1 scale for aggregation
- Considers: technical accuracy, mathematical rigor, constraint awareness, engineering judgment

**ğŸ“ˆ Final Scores**:
- Per-instance scores stored in `evaluations` table
- Aggregated scores (mean, CI) stored in `results` table
- Reports generated with statistical analysis

## ğŸ’» Usage

### ğŸš€ Launch Complete Evaluation

<div align="center">

**The launcher will start both agents and run a MechGAIA evaluation**

</div>

```bash
# Launch complete evaluation
# Default: Runs one instance of Level C for testing
uv run python main.py launch

# Run a specific level (A, B, C, or D)
uv run python main.py launch --level A
# or using short form
uv run python main.py launch -l A

# Run multiple levels
uv run python main.py launch --levels A,B,C
```

<div align="center">

| Option | Short Form | Description |
|--------|------------|-------------|
| `--level` | `-l` | Run a single task level (A, B, C, or D) |
| `--levels` | - | Run multiple levels as comma-separated values (e.g., `A,B,C`) |
| *(none)* | - | Defaults to one instance of Level C for testing |

</div>

### ğŸ”„ Run Agents Separately

<div align="center">

**You can also run the agents separately for more control**

</div>

```bash
# Start green agent (assessment manager) on port 9001
uv run python main.py green

# Start white agent (target being tested) on port 9002
uv run python main.py white
```

### ğŸŒ Remote Evaluation

<div align="center">

**If you have agents running remotely**

</div>

```bash
# Basic remote evaluation
uv run python main.py launch_remote <green_url> <white_url>

# Remote evaluation with specific level(s)
uv run python main.py launch_remote <green_url> <white_url> --level A
uv run python main.py launch_remote <green_url> <white_url> --levels A,B,C

# Remote evaluation with custom model
uv run python main.py launch_remote <green_url> <white_url> --model openai/gpt-4o
```

### ğŸ“ Task Generation

<div align="center">

**Generate tasks for evaluation**

</div>

```bash
# Generate Level A tasks
python scripts/generate_tasks.py generate --level A --num-tasks 5 --num-instances 10

# Generate Level B tasks
python scripts/generate_tasks.py generate --level B --num-tasks 5 --num-instances 10

# Generate Level C tasks
python scripts/generate_tasks.py generate --level C --num-tasks 3 --num-instances 10

# Load Level D tasks from examples
python scripts/generate_tasks.py generate --level D --examples-dir data/level_d/examples
```

### ğŸ“Š Result Analysis

<div align="center">

**Generate reports from evaluation results**

</div>

```bash
# Analyze results and generate report
python scripts/analyze_results.py analyze --output-dir results

# View markdown report
cat results/report.md

# View JSON results
cat results/results.jsonl
```

## âš™ï¸ Configuration

### ğŸ” Environment Variables

<div align="center">

**Create a `.env` file in the project root**

</div>

```bash
# OpenAI API Configuration
# Both green and white agents use the same API key
OPENAI_API_KEY=sk-your-openai-api-key-here

# Optional: for litellm proxy
LITELLM_PROXY_API_KEY=your-proxy-key

# Optional: Agent URL Configuration (defaults to localhost if not set)
AGENT_URL_GREEN=http://localhost:9001
AGENT_URL_WHITE=http://localhost:9002
```

> **ğŸ’¡ Note**: If `AGENT_URL_GREEN` or `AGENT_URL_WHITE` are not set, the agents will automatically default to `http://localhost:{port}` based on the port they're running on.

### ğŸ—„ï¸ Database Configuration

<div align="center">

**Tasks and results are stored in SQLite database**

</div>

| Setting | Value |
|---------|-------|
| **Default Location** | `data/benchmark.db` |
| **Tables** | `tasks`, `task_instances`, `evaluations`, `results` |

**Table Descriptions**:
- ğŸ“‹ `tasks`: Task definitions (schema, level, topic)
- ğŸ”¢ `task_instances`: Specific task instantiations with parameters
- âœ… `evaluations`: Individual evaluation results
- ğŸ“Š `results`: Aggregated scores per task

### ğŸ¤– Agent Configuration

<div align="center">

| Agent | Configuration File |
|-------|-------------------|
| ğŸŸ¢ **Green Agent** | `src/green_agent/mechgaia_green_agent.toml` |
| âšª **White Agent** | `src/white_agent/general_white_agent.toml` |

</div>

## ğŸ§ª Testing

<div align="center">

**The project includes a comprehensive test suite using pytest**

</div>

```bash
# Run all tests
uv run pytest tests/

# Run tests with verbose output
uv run pytest tests/ -v

# Run a specific test file
uv run pytest tests/test_schema_validation.py

# Run a specific test
uv run pytest tests/test_schema_validation.py::test_level_a_schema_validation
```

<div align="center">

**Test Coverage**

</div>

| Test Area | Coverage |
|-----------|----------|
| âœ… Schema validation | All levels (A, B, C, D) |
| ğŸ”¢ Numeric verifiers | Level B/C/D tasks |
| ğŸ“‹ JSON parsing | Response extraction |
| ğŸ—ï¸ Level D examples | Example validation |
| ğŸ¤– White agent | Output parsing |

## ğŸ”§ Troubleshooting

### âŒ Common Issues

<details>
<summary><b>ğŸ” "No tasks found"</b></summary>

**Solution**:
- Run `scripts/generate_tasks.py` first to populate the database
- Check that `data/benchmark.db` exists and contains tasks

</details>

<details>
<summary><b>ğŸ”Œ "Connection refused"</b></summary>

**Solution**:
- Make sure both agents are running
- Check ports 9001 (green) and 9002 (white) are available
- Verify firewall settings if running remotely

</details>

<details>
<summary><b>ğŸ”‘ "API key error"</b></summary>

**Solution**:
- Set `OPENAI_API_KEY` environment variable
- Create `.env` file in project root with your API key
- Verify API key is valid and has sufficient credits

</details>

<details>
<summary><b>ğŸ“‹ "No Level C tasks found"</b></summary>

**Solution**:
- Generate Level C tasks: `python scripts/generate_tasks.py generate --level C --num-tasks 3 --num-instances 10`
- Check database: `sqlite3 data/benchmark.db "SELECT * FROM tasks WHERE level='C';"`

</details>

<details>
<summary><b>â±ï¸ "Agent not ready in time"</b></summary>

**Solution**:
- Increase timeout in `src/my_util/my_a2a.py` if agents are slow to start
- Check agent logs for startup errors
- Verify dependencies are installed correctly

</details>

<details>
<summary><b>ğŸ’» "Code execution failed"</b></summary>

**Solution**:
- Check white agent's code output format
- Verify Python code syntax is correct
- Check sandbox executor logs for execution errors

</details>

### ğŸ› Debugging Tips

<div align="center">

**Helpful debugging strategies**

</div>

1. **ğŸ“‹ Check Agent Logs**: Both agents print detailed logs to console
2. **ğŸ—„ï¸ Database Inspection**: Use SQLite to inspect tasks and results:
   ```bash
   sqlite3 data/benchmark.db
   .tables
   SELECT * FROM tasks LIMIT 5;
   SELECT * FROM evaluations LIMIT 5;
   ```
3. **ğŸ”¬ Test Individual Components**: Run agents separately to isolate issues
4. **âœ… Verify Environment**: Ensure all dependencies are installed (`uv sync`)

## ğŸ“ Project Structure

<div align="center">

**Project organization and key directories**

</div>

```
src/
â”œâ”€â”€ green_agent/          # ğŸŸ¢ Assessment manager agent (hosts MechGAIA benchmark)
â”‚   â”œâ”€â”€ agent.py          # Green agent implementation
â”‚   â””â”€â”€ mechgaia_green_agent.toml  # Agent configuration
â”œâ”€â”€ white_agent/          # âšª Target agent being tested
â”‚   â”œâ”€â”€ agent.py          # White agent implementation
â”‚   â””â”€â”€ general_white_agent.toml    # Agent configuration
â”œâ”€â”€ launcher.py           # ğŸš€ Evaluation coordinator
â”œâ”€â”€ mechgaia_env/         # ğŸ—ï¸ MechGAIA environment implementation
â”‚   â”œâ”€â”€ database.py       # Database operations
â”‚   â”œâ”€â”€ evaluators.py     # Scoring and evaluation logic
â”‚   â”œâ”€â”€ env.py            # Environment interface
â”‚   â”œâ”€â”€ task_generator.py # Task generation
â”‚   â””â”€â”€ ...
â””â”€â”€ my_util/              # ğŸ› ï¸ Utility functions
    â””â”€â”€ my_a2a.py         # A2A communication helpers

scripts/
â”œâ”€â”€ generate_tasks.py     # ğŸ“ Task generation script
â”œâ”€â”€ analyze_results.py    # ğŸ“Š Result analysis script
â””â”€â”€ ...

data/
â”œâ”€â”€ benchmark.db          # ğŸ—„ï¸ SQLite database (tasks and results)
â”œâ”€â”€ level_c/              # Level C task examples
â”œâ”€â”€ level_d/              # Level D task examples
â””â”€â”€ materials.json        # ğŸ”¬ Material properties database

tests/                    # ğŸ§ª Test suite
results/                  # ğŸ“Š Evaluation results and reports
```

## ğŸ—ï¸ MechGAIA Environment

<div align="center">

**The framework uses the MechGAIA benchmark environment (`env: "mechgaia"`)**

</div>

### âœ¨ Features

| Feature | Description |
|---------|-------------|
| ğŸ“‹ **Problem Scenarios** | Realistic engineering problems across multiple difficulty levels |
| ğŸ› ï¸ **Scientific Computing Tools** | Calculator, Python scipy, numpy, material database, plotting tools |
| ğŸ—„ï¸ **Task Management** | Database-backed task storage and retrieval |
| âœ… **Evaluation Framework** | Automated quantitative and qualitative scoring |

### ğŸ› ï¸ Available Tools

- ğŸ”¢ **Calculator**: Basic arithmetic operations
- ğŸ **Python scipy**: Scientific computing
- ğŸ“Š **Python numpy**: Numerical operations
- ğŸ”¬ **Material Database**: Engineering properties
- ğŸ“ˆ **Plotting Tools**: Stress-strain and other diagrams

> The green agent instantiates MechGAIA to test the white agent's ability to solve mechanical engineering problems using the available tools.

## ğŸ¯ Key Points

<div align="center">

**Quick reference guide**

</div>

| Component | Description |
|-----------|-------------|
| ğŸŸ¢ **Green Agent** | Evaluator/Assessor (manages benchmark, evaluates responses) |
| âšª **White Agent** | Agent Being Tested (the LLM that solves tasks) |
| ğŸ—„ï¸ **Database** | Tasks stored in SQLite (`data/benchmark.db`) |
| ğŸ“Š **Results** | Evaluated and stored automatically |
| ğŸ“ˆ **Reports** | Use `scripts/analyze_results.py` to generate statistical reports |
| ğŸš€ **Default Launch** | Runs one instance of Level C for quick testing |
| ğŸ“‹ **Level Specification** | Explicit level evaluates all instances for that level |

---

## ğŸ“„ License

<div align="center">

[Add license information here]

</div>

## ğŸ¤ Contributing

<div align="center">

[Add contributing guidelines here]

**We welcome contributions!** Please feel free to submit a Pull Request.

</div>

## ğŸ“š Citation

<div align="center">

[Add citation information here]

</div>

---

<div align="center">

**Made with â¤ï¸ for the Mechanical Engineering AI Community**

[â¬† Back to Top](#mechgaia-agentbeats)

</div>
